{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing Rebuild\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func: Callable, input):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(10000, 10000).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "\n",
    "def square_3(a):\n",
    "    return a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pytorch_function(torch.square, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pytorch_function(torch.square, square_2(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pytorch_function(torch.square, square_3(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default way to use profiler\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for _ in range(10):\n",
    "        a = torch.square(torch.randn(10000, 10000).cuda())\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trace_dl(trace_path: str) -> None:\n",
    "    import json\n",
    "    import base64\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    with open(trace_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        json_str = json.dumps(data)\n",
    "    b64 = base64.b64encode(json_str.encode()).decode()\n",
    "\n",
    "    # Create a download link\n",
    "    href = f'<a href=\"data:text/json;base64,{b64}\" download=\"data.json\">Download JSON</a>'\n",
    "    return HTML(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = create_trace_dl(\"./trace.json\")\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Default way to use profiler\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "#     for _ in range(10):\n",
    "#         a = torch.square(torch.randn(10000, 10000).cuda())\n",
    "\n",
    "# prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "\n",
    "## With warmup and skip\n",
    "# https://pytorch.org/docs/stable/profiler.html\n",
    "\n",
    "\n",
    "# Non-default profiler schedule allows user to turn profiler on and off\n",
    "# on different iterations of the training loop;\n",
    "# trace_handler is called every time a new trace becomes available\n",
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n",
    "    on_trace_ready=trace_handler,\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    ") as p:\n",
    "    for iter in range(10):\n",
    "        torch.square(torch.randn(10000, 10000).cuda())\n",
    "        # send a signal to the profiler that the next iteration has started\n",
    "        p.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPP extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_source = \"\"\"\n",
    "std::string hello_world() {\n",
    "  return \"Hello, world\";\n",
    "}\n",
    "\"\"\"\n",
    "cpp_source2 = \"\"\"\n",
    "std::string goodbye_world() {\n",
    "  return \"Goodbye world\";\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_module = load_inline(\n",
    "#     name=\"my_module\",\n",
    "#     cpp_sources=[cpp_source, cpp_source2],\n",
    "#     functions=[\"hello_world\", \"goodbye_world\"],\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /root/.cache/torch_extensions/py311_cu121/my_module/main.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_module.hello_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "triton_path = \"/Users/peterbull/peter-projects/gpu-mode/lectures/triton/python\"\n",
    "if os.path.exists(triton_path) and triton_path not in sys.path:\n",
    "    sys.path.append(triton_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted straight from https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "# if @triton.jit(interpret=True) does not work, please use the following two lines to enable interpret mode\n",
    "# import os\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "\n",
    "\n",
    "@triton.jit(interpret=True)\n",
    "def square_kernel(\n",
    "    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    # The rows of the softmax are independent, so we parallelize across those\n",
    "    row_idx = tl.program_id(0)\n",
    "    # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "    # row in a single block\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_ptrs = row_start_ptr + col_offsets\n",
    "    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float(\"inf\"))\n",
    "\n",
    "    square_output = row * row\n",
    "\n",
    "    # Write back output to DRAM\n",
    "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + col_offsets\n",
    "    tl.store(output_ptrs, square_output, mask=col_offsets < n_cols)\n",
    "\n",
    "\n",
    "def square(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    # The block size is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n",
    "    # f the input matrix\n",
    "    square_kernel[(n_rows,)](\n",
    "        y,\n",
    "        x,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_cols,\n",
    "        num_warps=num_warps,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    return y\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=\"cuda\")\n",
    "y_triton = square(x)\n",
    "y_torch = torch.square(x)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"N\"],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=[\"triton\", \"torch-native\", \"torch-compile\"],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch (native)\", \"Torch (compiled)\"],  # label name for the lines\n",
    "        styles=[(\"blue\", \"-\"), (\"green\", \"-\"), (\"green\", \"--\")],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"square() performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={\"M\": 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch-native\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.square(x), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: square(x), quantiles=quantiles)\n",
    "    if provider == \"torch-compile\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.compile(torch.square)(x), quantiles=quantiles\n",
    "        )\n",
    "    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True, save_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
